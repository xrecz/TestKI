Perfekt, Darius — hier ist die komplette Schritt-für-Schritt-Anleitung für Ubuntu, mit der du den Agenten komplett offline (nach Erst-Download) betreibst. Alles ist so formuliert, dass du die Befehle direkt in die Shell kopieren kannst. Am Ende steht genau, was du ggf. anpassen musst.

1) Zielbild

Du lädst ein GGUF-Modell (z. B. Qwen2.5-7B-Instruct Q5_K_M) in einen lokalen llama.cpp-Server auf 127.0.0.1:8080.

Dein Agent (agent.py) spricht OpenAI-kompatibel mit diesem lokalen Server.

Tools: Shell, Datei-I/O, Excel lesen/auswerten/exportieren.

Nach der Einrichtung kannst du die Maschine vom Netz trennen → alles bleibt lokal.

2) Voraussetzungen (einmalig)
sudo apt update
sudo apt install -y python3-venv build-essential cmake jq
mkdir -p ~/offline-agent && cd ~/offline-agent
python3 -m venv .venv && source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install "llama-cpp-python[server]" smolagents pandas openpyxl
# optional (nur für bequemes Modell-Downloaden):
pip install -U huggingface_hub


Tipp: Für Repo-Suchen kannst du später noch sudo apt install -y ripgrep installieren. Nicht nötig, aber praktisch.

3) Modell (GGUF) lokal ablegen

Wir nehmen Qwen2.5-7B-Instruct Q5_K_M (gute Qualität; ~5–6 GB Datei; ~8–10 GB RAM empfehlenswert).
Wenn du knapperen RAM hast, nimm Q4_K_M statt Q5_K_M.

mkdir -p ./models/qwen2.5-7b
# Lade genau diese Datei:
huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF \
  --include "Qwen2.5-7B-Instruct-Q5_K_M.gguf" \
  --local-dir ./models/qwen2.5-7b --local-dir-use-symlinks False


Alternative ohne hf-cli: Falls du die .gguf schon hast:

mkdir -p ./models/qwen2.5-7b
cp /pfad/zu/Qwen2.5-7B-Instruct-Q5_K_M.gguf ./models/qwen2.5-7b/

4) Lokalen LLM-Server starten (bleibt auf localhost)
python -m llama_cpp.server \
  --model ./models/qwen2.5-7b/Qwen2.5-7B-Instruct-Q5_K_M.gguf \
  --alias local-gguf \
  --port 8080 \
  --n_threads $(nproc) \
  --n_ctx 8192


Funktionstest (neues Terminal):

curl -s http://127.0.0.1:8080/v1/models | jq


Du solltest ein Model mit Name local-gguf sehen.

Leistungstipps:

Weniger RAM? --n_ctx 4096.

Noch flotter? Später auf Q4_K_M wechseln.

CPU-Kerne begrenzen? --n_threads 8 (statt $(nproc)).

5) Agent-Datei anlegen

Lege agent.py in ~/offline-agent/ an:

# agent.py — Interaktiver Offline-Agent (Ubuntu/CPU)
from smolagents import CodeAgent, OpenAIServerModel, tool
from subprocess import run
from typing import Optional, List
import sys, os, json, io, contextlib
import pandas as pd

@tool
def sh(cmd: str) -> str:
    """Run a shell command.
    Args:
        cmd (str): Command line to execute via the system shell.
    Returns:
        str: Combined stdout+stderr.
    """
    try:
        r = run(cmd, shell=True, capture_output=True, text=True, timeout=60)
        return (r.stdout or "") + (r.stderr or "")
    except Exception as e:
        return f"error:{e}"

@tool
def read_file(path: str, max_chars: int = 20000) -> str:
    """Read a UTF-8 text file (truncated).
    Args:
        path (str): File path.
        max_chars (int): Max chars to return.
    Returns:
        str: File content.
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = f.read()
        return data[:max_chars] + ("...[truncated]" if len(data) > max_chars else "")
    except Exception as e:
        return f"error:{e}"

@tool
def write_file(path: str, content: str) -> str:
    """Write UTF-8 text to a file.
    Args:
        path (str): Target file path.
        content (str): Text to write.
    Returns:
        str: Status.
    """
    try:
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"saved:{path}"
    except Exception as e:
        return f"error:{e}"

@tool
def read_excel(path: str, sheet: Optional[str] = None, head: int = 5) -> str:
    """Load Excel and return structure+preview as JSON.
    Args:
        path (str): Excel file path.
        sheet (str, optional): Sheet name or None.
        head (int): Preview rows.
    Returns:
        str: JSON with rows, columns, dtypes, na_counts, preview_markdown.
    """
    try:
        df = pd.read_excel(path, sheet_name=sheet, engine="openpyxl")
        info = {
            "rows": int(len(df)),
            "columns": list(map(str, df.columns)),
            "dtypes": {c: str(df[c].dtype) for c in df.columns},
            "na_counts": {c: int(df[c].isna().sum()) for c in df.columns},
            "preview_markdown": df.head(head).to_markdown(index=False)
        }
        return json.dumps(info, ensure_ascii=False)
    except Exception as e:
        return f"error:{e}"

@tool
def excel_groupby(path: str, sheet: Optional[str], by: List[str], value: str, agg: str = "sum") -> str:
    """Group Excel by columns and aggregate a value column.
    Args:
        path (str): Excel file path.
        sheet (str, optional): Sheet name or None.
        by (list[str]): Group-by columns.
        value (str): Value column.
        agg (str): Aggregation ('sum','mean','count',...).
    Returns:
        str: Markdown table.
    """
    try:
        df = pd.read_excel(path, sheet_name=sheet, engine="openpyxl")
        g = df.groupby(by)[value].agg(agg).reset_index()
        return g.to_markdown(index=False)
    except Exception as e:
        return f"error:{e}"

@tool
def to_csv_from_excel(path: str, sheet: Optional[str], out_csv: str) -> str:
    """Export Excel sheet to CSV.
    Args:
        path (str): Excel file path.
        sheet (str, optional): Sheet name or None.
        out_csv (str): Output CSV path.
    Returns:
        str: Status.
    """
    try:
        df = pd.read_excel(path, sheet_name=sheet, engine="openpyxl")
        df.to_csv(out_csv, index=False)
        return f"saved:{out_csv}"
    except Exception as e:
        return f"error:{e}"

# Lokales LLM: llama.cpp-Server (OpenAI-kompatibel)
model = OpenAIServerModel(
    model_id="local-gguf",                  # muss zu --alias passen
    api_base="http://127.0.0.1:8080/v1",    # lokaler Server
    api_key="sk-local",                     # Dummy, wird nicht geprüft
    temperature=0.2,
    max_tokens=1024,
)

agent = CodeAgent(
    model=model,
    tools=[sh, read_file, write_file, read_excel, excel_groupby, to_csv_from_excel],
    add_base_tools=False
)

GUIDANCE = (
    "Use sh to explore the filesystem. "
    "Use read_file/write_file for text; read_excel/excel_groupby/to_csv_from_excel for spreadsheets."
)

def main() -> int:
    print("🔒 Offline-Agent bereit (localhost). 'exit' zum Beenden.")
    prompt = input("👉 Prompt: ").strip()
    if not prompt or prompt.lower() in ("exit","quit"):
        print("Abbruch."); return 0
    print("\n⏳ Arbeite…\n")
    out = agent.run(prompt + "\n\n" + GUIDANCE)
    print("\n✅ Ergebnis:\n")
    print(out)
    return 0

if __name__ == "__main__":
    sys.exit(main())

6) Agent starten

Terminal A: (Server laufen lassen)

python -m llama_cpp.server \
  --model ./models/qwen2.5-7b/Qwen2.5-7B-Instruct-Q5_K_M.gguf \
  --alias local-gguf --port 8080 --n_threads $(nproc) --n_ctx 8192


Terminal B: (Agent)

cd ~/offline-agent
source .venv/bin/activate
python agent.py
# 👉 Prompt-Beispiele:
# Lies 'sales.xlsx' (Sheet 'Q2'), zeige NA-Counts und bilde Umsatz je Region (sum).
# Liste Dateien im aktuellen Ordner und schreibe die Liste in files.txt.
# Erstelle ein Bash-Skript build.sh, das pytest ausführt, speichere es und führe es aus.

7) „Wirklich offline“ danach

Wenn alles läuft und die .gguf lokal liegt:

export HF_HUB_OFFLINE=1
# Optional: VM/Server vom Netz trennen – Agent & Server bleiben funktionsfähig.


Air-gap (voll offline auf Zielsystem):

Auf einer Online-Kiste:

echo -e "llama-cpp-python[server]\nsmolagents\npandas\nopenpyxl" > requirements.txt
pip download -r requirements.txt -d wheelhouse
# .gguf nach ./models/qwen2.5-7b/ legen


Auf der Offline-VM:

python3 -m venv .venv && source .venv/bin/activate
pip install --no-index --find-links=wheelhouse -r requirements.txt
# Server & Agent wie oben starten

8) Optional: Autostart des Servers (systemd)
sudo tee /etc/systemd/system/llamacpp.service >/dev/null <<'UNIT'
[Unit]
Description=Local llama.cpp server
After=network.target

[Service]
Type=simple
User=%i
WorkingDirectory=/home/%i/offline-agent
ExecStart=/home/%i/offline-agent/.venv/bin/python -m llama_cpp.server \
  --model /home/%i/offline-agent/models/qwen2.5-7b/Qwen2.5-7B-Instruct-Q5_K_M.gguf \
  --alias local-gguf --port 8080 --n_threads $(nproc) --n_ctx 8192
Restart=on-failure

[Install]
WantedBy=multi-user.target
UNIT

# Dienst aktivieren (ersetze <user> durch deinen Benutzernamen):
sudo systemctl enable llamacpp.service
sudo systemctl start llamacpp.service
sudo systemctl status llamacpp.service

9) Was du ggf. anpassen musst

Modellpfad & Dateiname:
In Schritt 4 (Server-Start) und im systemd-Unit muss der Pfad zu deiner .gguf passen.

Alias/Port:
Wenn du --alias oder --port änderst, passe in agent.py die Felder model_id bzw. api_base an.

Ressourcen:
Zu wenig RAM/zu langsam → nimm Qwen2.5-7B-Instruct-Q4_K_M.gguf oder reduziere --n_ctx (z. B. 4096).

Excel-Dateinamen/Sheets:
In deinen Prompts real existierende Pfade/Sheets verwenden (z. B. ./data/sales.xlsx, Sheet-Name korrekt).

10) Troubleshooting (kurz)

No module named llama_cpp.server → venv aktiv? pip install "llama-cpp-python[server]" im gleichen venv.

Server antwortet nicht → curl 127.0.0.1:8080/v1/models, Port/Firewall prüfen.

Speicher knapp → Q4_K_M oder kleineres Modell; --n_ctx senken.

Excel-Fehler → pip install -U pandas openpyxl; Pfad/Sheet checken.

Agent findet Modell nicht → alias im Server ≙ model_id im Agent.

Wenn du willst, gebe ich dir zusätzlich eine Miniliste an Test-Prompts für deinen Alltag (z. B. Log-Auswertung, CSV-Cleanup, MD-Report mit Ergebnissen).